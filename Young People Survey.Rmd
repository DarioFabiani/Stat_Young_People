---
title: "Young People Survey"
author: "Dario Fabiani, Beatrice Marsili"
subtitle: Statistical Models
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract
The purpose of this project is to analyze with different statistical methods the Young People Survey dataset (available at [Kaggle][id]).  
This dataset includes 1010 observations for 150 different varibles. It has been built over a questionary answered by Slovakian people aged between 15 and 30.  
These people were asked to respond questions regarding their Music and Movies preferences, their Hobbies and Phobias, their Spending and Health Habits, some of their Demographic characteristics and their thoughts about life.  
57 over 150 questions regarded this latter topic, thus we decided to try finding some patterns between these features, in order to utilze them to try predicting spending habits and other preferences expressed in the survey.   
We're in the scenario of *unsupervised learning*, as we have a set of $X_{1}, X_{2}, \dots, X_{57}$ features measured over 1010 observations, and we want to discover unknown subgroups within these features.   
We first had a look to these 57 questions and tried to understand which characteristics of personality were observed by each variable, simply using the Myers-Briggs indicator, often used in psichology. Obviously this method was very rough as was strictly dependant on our personal thoughts and could lead to misleading results.  We then went through a cluster analysis, a set of techniques that help discover hidden structures on the basis of a dataset. 

## Data Preparation
Our data was already technically correct. We renamed columns to avoid problems in handling them. 

```{r Column Rename}
library(stringr) 
df.responses <- read.csv("Data/responses.csv")
old <- names(df.responses)
new <- str_replace_all(old, "\\.", "_") 
new <- str_replace_all(new, "\\__", "_or_")  
new <- str_replace_all(new, "\\__", "_") 
colnames(df.responses) <- new
attach(df.responses)
```
We had to decide how to deal with NAs, that were the only special values in our dataset. 

```{r Na Values}
omit <- na.omit(df.responses)
dim(df.responses)[1] - dim(omit)[1]
```
Having 324 rows with Na values we decided that it wasn't a good idea to omit them, we then decided to fill those missing values with the median of the column. 
We choose to use the median as the variables are of ordered categorical type and choosing the mean would have changed the levels of our parameters.  
We filled Na values for the variables referred to Music, Movies, and Hobbies.

```{r Filling Na for preferences}
pref <- colnames(df.responses[0:63]) #columns for Music, Movies and Hobbies preferences
for(i in 1:length(pref)){
  df.responses[is.na(df.responses[,i]), i] <- median(df.responses[,i], na.rm = TRUE)
}
```
To handle properly the dataset we divided it into subsets.  
```{r Dividing into subsets and filling Na of personality}
df.music <- data.frame(df.responses[0:19])
df.movies <- data.frame(df.responses[20:31])
df.hobbies <- data.frame(df.responses[32:63])
df.phobias <- data.frame(df.responses[64:73])
df.health <- data.frame(df.responses[74:76])
df.personality <- data.frame(df.responses[77:133])
df.spending <- data.frame(df.responses[134:140])
df.demo <- data.frame(df.responses[140:150])
```
We then started to look at the data referred to personality questions, trasformed factors into numeric and handled NA. For this part of the dataset we preferred to substitute such values with *"3"*, as using the median as above would have affected correlation. 
```{r}
df.personality$Punctuality <- as.numeric(df.personality$Punctuality)
df.personality$Lying <- as.numeric(df.personality$Lying)
df.personality$Internet_usage <- as.numeric(df.personality$Internet_usage)

pers <- colnames(df.personality)

for(i in 1:length(pers)){
  df.personality[is.na(df.personality[,i]), i] <- 3
}

df.responses[77:133] <- df.personality
```
Before clustering our variables regarding personality to find possible personality types we wanted to visualize existing correlation inside this subset; we then  performed a correlation matrix and a correlation plot, that confirmed that there are some (positive and negative) patterns among the responses on such variables. 
```{r results="hide"}
cor(df.personality)
```

#QUESTO INTANTO L'HO MESSO AL MASSIMO LO LEVIAMO 
```{r Visually exploring correlations for personality, message=FALSE, fig.align="center"}
library(corrplot)
library(RColorBrewer)
pal <- colorRampPalette(c("#FC4E07", "#E7B800", "#00AFBB"))
corrplot(cor(df.personality, use = "complete.obs"), 
         type= "upper",
         method ="circle",
         col = pal(5),
         tl.cex = 0.38,
         tl.col = "black",
         tl.srt = 45,
         mar = c(2, 2, 2, 2), 
         main = "Correlation plot among personality variables",
         cex.main= 0.8,
         col.main="black")
```

## Hierarchical Clustering 
Even if we had some ideas about the number of clusters to be obtained (on the basis of the rough analysis done with Myers-Briggs indicator) we decided to use a **Hierarchical Cluster**, without definig *a priori* the number of cluster to be obtained.  
Considering the types of our features, ordered categorical, we decided to use as a **measure of distance** a *correlation based* one, as using the classical *Euclidean distance* would have been meaningless. Furthermore, the *Euclidean distance* would have tried to discover patterns among the observation, while we are interested in patterns among the features.  As **linkage** method, after having a look at the results obtained with *Average, Complete and Single* linkage methods we decided to use the *Ward* one, that considers, merging two cluster A and B, the measure of how much the sum of squares would increase when merging them and minimizes this measure, called **merging cost**. 
```{r Ward method based clustering,fig.dim=c(10,8), fig.align="center", message=FALSE}
library(factoextra)
library(ggplot2)

hc.ward <- hclust(as.dist(1-cor(df.personality)), method = "ward.D")
fviz_dend(hc.ward, main = "Ward Linkage - Personality", cex = .5, lwd = 0.4)
```

Looking at the above result we decided to cut our dendogram to obtain 5 clusters, we then added 5 columns to the dataset, one for each cluster, and filled the values with scores obtained by each observation as a mean of the responses given to the variables belonging to the different clusters. 
```{r 5 cluster visualization, fig.dim=c(10,8), fig.align="center"}
fviz_dend(hc.ward, main = "Ward Linkage - Personality", 
          cex = .5,  
          lwd = 0.4,
          k = 5, 
          color_labels_by_k = TRUE, 
          rect = T, 
          k_colors = c("#FC4E07", "#E7B800", "#00AFBB", "#fda683", "#005ebb"))

ward.cut <- cutree(hc.ward, k=5)

df.responses$Cluster_1 <- NA
df.responses$Cluster_2 <- NA
df.responses$Cluster_3 <- NA
df.responses$Cluster_4 <- NA
df.responses$Cluster_5 <- NA

cl_1 <- names(ward.cut[ward.cut==1])
cl_2 <- names(ward.cut[ward.cut==2])
cl_3 <- names(ward.cut[ward.cut==3])
cl_4 <- names(ward.cut[ward.cut==4])
cl_5 <- names(ward.cut[ward.cut==5])

df.responses$Cluster_1 <- round(rowMeans(df.responses[cl_1]),2)
df.responses$Cluster_2 <- round(rowMeans(df.responses[cl_2]),2)
df.responses$Cluster_3<- round(rowMeans(df.responses[cl_3]),2)
df.responses$Cluster_4 <- round(rowMeans(df.responses[cl_4]),2)
df.responses$Cluster_5 <- round(rowMeans(df.responses[cl_5]),2)
```
## Principal Component Analysis
Since our dataset has a lot of variables we performed a PCA on the different dataset realised dividing the origianl one in different topics. We decided to perform separated for each topic beacuse we wanted to keep as much information as possible. 
Unfortunatly the principal component analysis for the majority of the subsets is not very informative. As can be seen in the secondary submitted markdown, for the Movies preferences in example, we should use 8 component out of the 10 original ones to explain the 80%.  
```{r PCA for Spending Habits, message=FALSE, fig.align="center"}
spend <- colnames(df.spending) #columns for Spending Habits
for(i in 1:length(spend)){
  df.spending[is.na(df.spending[,i]), i] <- median(df.spending[,i], na.rm = TRUE)
}

pca.spending <- prcomp(df.spending, scale = TRUE,)
pca.spending$rotation <- -pca.spending$rotation
pca.spending$x <- -pca.spending$x

fviz_pca_var(pca.spending,
             col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, 
             title="PCA for Spending Habits"
)
```

```{r Scree Plot and Cumulative Variance Explained Visualization (Spending Habits), fig.align="center"}
pv.spending <- pca.spending$sdev^2 
pvp.spending <- pv.spending/sum(pv.spending)

spe <- fviz_eig(pca.spending,
         addlabels = T, 
         barcolor = "#E7B800", 
         barfill = "#E7B800", 
         linecolor = "#00AFBB", 
         choice = "variance",
         title = "Scree Plot - Spending Habits",
         ylim=c(0,45)) 

spe.cumsum <- data.frame(x=1:length(pvp.spending),
                 y=cumsum(pvp.spending)*100/4)
spe.tot <- spe  + 
     geom_point(data=spe.cumsum, aes(x, y), size=1, color="#00AFBB") +
     geom_line(data=spe.cumsum, aes(x, y), color="#00AFBB") +
     scale_y_continuous(sec.axis = sec_axis(~ . * 4, 
                                   name = "Cumulative proportion of Variance Explained") )
print(spe.tot)
```
For our research constraint, by the way, we are mainly interested in the spending habits, and for this subset the results are quite better: 4 components explain the 80% of the total variance. The elbow appears to be after the first component, that taken alone explains the 50% of the variance. We could think of taking this first one and the second as they're quite representative (more than 60% of the variance)

[id]: https://www.kaggle.com/miroslavsabo/young-people-survey /"Kaggle"